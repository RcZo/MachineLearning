{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BANK_0902.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9In3lom2wYqU"
      },
      "source": [
        "## DATA Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56084uZG8Vt8"
      },
      "source": [
        "cd /home"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6drmzkR8YJr"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOWqftrFYHn8"
      },
      "source": [
        "import pandas as pd\n",
        "import os.path\n",
        "import io\n",
        "from google.colab import files\n",
        "\n",
        "if(os.path.isfile('bank-additional-full.csv')):\n",
        "  data = pd.read_csv('bank-additional-full.csv',sep=';')\n",
        "  print('File ready.')\n",
        "else:\n",
        "  print('File not found, please reuplod.')\n",
        "  uploaded = files.upload()\n",
        "  data = pd.read_csv('bank-additional-full.csv',sep=';')\n",
        "  print('File ready.') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wLamGWm5k2M"
      },
      "source": [
        "## DATA Description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z83W7bSJstEG"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx-aqFzvtGQ0"
      },
      "source": [
        "#Seperate categorical data & numeric data\n",
        "categoyData=data.loc[:, data.dtypes == object]\n",
        "numericData=data.loc[:, data.dtypes != object]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUfiy6-mtJNj"
      },
      "source": [
        "#Describe categorical data\n",
        "categoyData.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba_xYdDrtKf0"
      },
      "source": [
        "#Describe numeric data\n",
        "numericData.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9dSfZEjPZv3"
      },
      "source": [
        "#Label encoding\n",
        "data['job']=pd.Categorical(data.job).codes\n",
        "data['marital']=pd.Categorical(data.marital).codes\n",
        "data['education']=pd.Categorical(data.education).codes \n",
        "data['default']=pd.Categorical(data.default).codes\n",
        "data['housing']=pd.Categorical(data.housing).codes\n",
        "data['loan']=pd.Categorical(data.loan).codes\n",
        "data['contact']=pd.Categorical(data.contact).codes\n",
        "data['month']=pd.Categorical(data.month).codes\n",
        "data['day_of_week']=pd.Categorical(data.day_of_week).codes\n",
        "data['poutcome']=pd.Categorical(data.poutcome).codes\n",
        "\n",
        "data=pd.get_dummies(data=data,columns='y')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ1b9p2KkKkz"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIRvfd6tkLcr"
      },
      "source": [
        "## Preprcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0CHs6Fs8C-X"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#Seperate feaures and classify target label\n",
        "labels= data[['y_no','y_yes']]\n",
        "features=data.drop(data[['y_no','y_yes']], axis=1)\n",
        "#Data standardization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(features) \n",
        "features= scaler.transform(features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSSsn9R48C-a"
      },
      "source": [
        "## K-Fold (for checking data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk9lL7rO8C-b"
      },
      "source": [
        "#Stratified K-Fold\n",
        "K_FOLD=10\n",
        "RANDOM_STATE=111"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhBKUFVF8C-h"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=K_FOLD,random_state=RANDOM_STATE)\n",
        "#Dataframe to arrays\n",
        "labels=labels.get_values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M9GCN8v8C-k"
      },
      "source": [
        "\"\"\" \n",
        "----------------------------------------------\n",
        "|                               |          |\n",
        "|                               |          |\n",
        "|                               |          |\n",
        "|         X_train               |  y_train |\n",
        "|                               |          |\n",
        "|                               |          |\n",
        "|                               |          |\n",
        "|------------------------------------------|\n",
        "|          X_test               |  y_test  |\n",
        "---------------------------------------------\n",
        "\"\"\" \n",
        "\n",
        "#For testing, slice data by index\n",
        "for train_index, test_index in skf.split(features, labels[0:,0]):\n",
        "    X_train, X_test = features[train_index], features[test_index]    \n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xehwrU3anvrC"
      },
      "source": [
        "## Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOTbHbXuFnB"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p79kOLOJx8gZ"
      },
      "source": [
        "## Hyper parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMZHsQB1vRmn"
      },
      "source": [
        "N_INPUT =len(features[0])\n",
        "N_CLASSES =len(labels[0])\n",
        "\n",
        "LEARNING_RATE = 0.001                               \t\t\t  \n",
        "TRAINING_EPOCHS = 500                               \t\t\t    \n",
        "DISPLAY_STEP = 10                                    \t\t\t    \n",
        "STDDEV = 0.1  #Distribution of initial weight & baias                                \t\t\t   \n",
        "\t\t\t\t\t\t                            \n",
        "DROP_OUT=1.0\n",
        "\n",
        "ACTVATION=tf.nn.tanh \n",
        "#tf.nn.sigmoid\n",
        "#tf.nn.relu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYBDMIOcxx5Q"
      },
      "source": [
        "n_input = N_INPUT       # input\n",
        "n_hidden_1 = 4          # 1st hidden layer\n",
        "n_hidden_2 = 4          # 2nd hidden layer\n",
        "n_hidden_3 = 4          # 3rd hidden layer\n",
        "n_classes = N_CLASSES   # output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfBF5aCcyFvg"
      },
      "source": [
        "## Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfE6VzwZx6dI"
      },
      "source": [
        "#Data entry \n",
        "X = tf.placeholder(tf.float32, [None, n_input])\n",
        "y = tf.placeholder(tf.float32, [None, n_classes])\n",
        "dropout_keep_prob = tf.placeholder(tf.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPEl9fojyLuc"
      },
      "source": [
        "## MLP Perceptron part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAnt0k65ewW8"
      },
      "source": [
        "#MLP define\n",
        "def mlp(_X, _weights, _biases, dropout_keep_prob):\n",
        "    layer1 = tf.nn.dropout( ACTVATION(tf.add(tf.matmul(_X, _weights['w1']), _biases['b1'])), dropout_keep_prob)\n",
        "    layer2 = tf.nn.dropout( ACTVATION(tf.add(tf.matmul(layer1, _weights['w2']), _biases['b2'])), dropout_keep_prob)\n",
        "    layer3 = tf.nn.dropout( ACTVATION(tf.add(tf.matmul(layer2, _weights['w3']), _biases['b3'])), dropout_keep_prob)\n",
        "    out = ACTVATION(tf.add(tf.matmul(layer3, _weights['wout']), _biases['bout']))\n",
        "    return out\n",
        "\n",
        "weights = {\n",
        "    'w1': tf.Variable(tf.random_normal([n_input, n_hidden_1],stddev=STDDEV)),\n",
        "    'w2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],stddev=STDDEV)),\n",
        "    'w3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3],stddev=STDDEV)),\n",
        "    'wout': tf.Variable(tf.random_normal([n_hidden_3, n_classes],stddev=STDDEV))                                  \n",
        "}\n",
        "\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'bout': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28mGIwYzzlL8"
      },
      "source": [
        "pred = mlp(X, weights, biases, dropout_keep_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuWl7Cbyj7D"
      },
      "source": [
        "## Cost function & Gradient Decent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckrTeBx_z4CL"
      },
      "source": [
        "# Cost function & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate = LEARNING_RATE).minimize(cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcIFfdvb8C-8"
      },
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnIA3Dnn8C-9"
      },
      "source": [
        "\"\"\"\"\"\n",
        "                    output\n",
        "                |  0  |  1      \n",
        "-----------------------------\n",
        "            | 0 | TN  | FP\n",
        "desire ----------------------\n",
        "            | 1 | FN  | TP\n",
        "         \n",
        "Precision=tp/tp+fp  \n",
        "Recall=tp/tp+fn\n",
        "\n",
        "\"\"\"\"\"\n",
        "#Confusion matrix\n",
        "confmat=tf.confusion_matrix(tf.argmax(pred, 1),tf.argmax(y, 1),num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdkeE466zIty"
      },
      "source": [
        "## Graph session Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j59t7fAPcp4Z"
      },
      "source": [
        "#Timer\n",
        "import time\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WGRWMAi8C_Q"
      },
      "source": [
        "history=[]\n",
        "tStart = time.time()\n",
        "currentFold=0\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(features, labels[0:,0]):\n",
        "    currentFold=currentFold+1\n",
        "    X_train, X_test = features[train_index], features[test_index]    \n",
        "    y_train, y_test = labels[train_index], labels[test_index]\n",
        "    print(\"CurrentFold: \",currentFold)\n",
        "    \n",
        "    with tf.Session() as sess:      \n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        for epoch in range(TRAINING_EPOCHS):\n",
        "            TrainTP,TrainFP,TrainFN,TestTP,TestFP,TestFN=0,0,0,0,0,0\n",
        "                        \n",
        "            _,costTrain,confusionTrain=sess.run([optimizer,cost,confmat], feed_dict={X: X_train, y: y_train,dropout_keep_prob: DROP_OUT})\n",
        "            \n",
        "                  \n",
        "            if (epoch+1) % (DISPLAY_STEP) == 0 :\n",
        "                costTest,confusionTest = sess.run([cost,confmat], feed_dict={X: X_test, y: y_test,dropout_keep_prob: 1.0})\n",
        "                TrainTP,TrainFP,TrainFN=confusionTrain[1,1],confusionTrain[0,1],confusionTrain[1,0]\n",
        "                TestTP,TestFP,TestFN=confusionTest[1,1],confusionTest[0,1],confusionTest[1,0]\n",
        "                                \n",
        "                FscoreTrain = 2*TrainTP/(2*TrainTP+TrainFP+TrainFN or not 0) #'or not' Avoid ZeroDivisionError\n",
        "                FscoreTest = 2*TestTP/(2*TestTP+TestFP+TestFN or not 0)\n",
        "                \n",
        "                history.append([currentFold,epoch+1,costTrain,costTest, FscoreTrain, FscoreTest])\n",
        "        \n",
        "        tEnd = time.time()\n",
        "        print(\"--------------------------------------------------Timer: %.5f sec----------------------------------------------------------\" %(tEnd-tStart))\n",
        "    sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDe2txS808Zp"
      },
      "source": [
        "## Draw plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70R7YqKD8C_T"
      },
      "source": [
        "#Draw plot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWDecPrlnEdP"
      },
      "source": [
        "#Plot frame\n",
        "frame=int(TRAINING_EPOCHS/DISPLAY_STEP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXRL5shU8C_X"
      },
      "source": [
        "#Loss curve\n",
        "for fold in range(K_FOLD):\n",
        "    plt.plot(np.transpose(history)[1,fold*frame:(fold+1)*frame],np.transpose(history)[2,fold*frame:(fold+1)*frame],linestyle='-')\n",
        "    plt.plot(np.transpose(history)[1,fold*frame:(fold+1)*frame],np.transpose(history)[3,fold*frame:(fold+1)*frame],linestyle='-')\n",
        "    plt.title('fold-'+str(fold+1))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['costTrain','costTest'], loc='upper right')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuKrZOwNuwwv"
      },
      "source": [
        "#F1 score curve\n",
        "for fold in range(K_FOLD):\n",
        "    plt.plot(np.transpose(history)[1,fold*frame:(fold+1)*frame],np.transpose(history)[4,fold*frame:(fold+1)*frame],linestyle='-')\n",
        "    plt.plot(np.transpose(history)[1,fold*frame:(fold+1)*frame],np.transpose(history)[5,fold*frame:(fold+1)*frame],linestyle='-')\n",
        "    plt.title('fold-'+str(fold+1))\n",
        "    plt.ylabel('F1score')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['Train','Test'], loc='lower right')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOOqMiArRf3A"
      },
      "source": [
        "#Result table\n",
        "for fold in range(K_FOLD):\n",
        "  MaxTrain = np.transpose(history)[4,fold*frame:(fold+1)*frame].max()\n",
        "  Maxloc=np.transpose(history)[4,fold*frame:(fold+1)*frame].argmax()\n",
        "  followingTest=np.transpose(history)[5,fold*frame:(fold+1)*frame][Maxloc]\n",
        "  print('fold-%d\\tBest Train F1 score: %.5f\\tepoch: %d\\tTest F1 score: %.5f' % ((fold+1),MaxTrain,(Maxloc+1)*DISPLAY_STEP,followingTest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm9Gn72ajV_G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}